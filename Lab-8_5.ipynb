{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nji1a9ULLtCA"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnsX1AWKLtCE"
   },
   "source": [
    "# Lab 8.5: Text Classification\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pm8PttyLtCI"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:33.182995Z",
     "start_time": "2019-06-17T01:38:30.045388Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EUANiH6zLtCK"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58bUNQA0LtCV"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqU7d_qcLtCX"
   },
   "source": [
    "Sample:\n",
    "\n",
    "    __label__2 Stuning even for the non-gamer: This sound ...\n",
    "    __label__2 The best soundtrack ever to anything.: I'm ...\n",
    "    __label__2 Amazing!: This soundtrack is my favorite m ...\n",
    "    __label__2 Excellent Soundtrack: I truly like this so ...\n",
    "    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n",
    "    __label__2 an absolute masterpiece: I am quite sure a ...\n",
    "    __label__1 Buyer beware: This is a self-published boo ...\n",
    "    . . .\n",
    "    \n",
    "There are only two **labels**:\n",
    "- `__label__1`\n",
    "- `__label__2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:42.024845Z",
     "start_time": "2019-06-17T01:38:41.896098Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rwWFJprZLtCZ"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "\n",
    "#../../IOD_Lab_Work_For_Github/Module_8/corpus.txt\n",
    "\n",
    "trainDF = pd.read_fwf(\n",
    "    filepath_or_buffer = '../../IOD_Lab_Work_For_Github/Module_8/corpus.txt',\n",
    "    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n",
    "                (11, 9000) # text: makes the it big enought to get to the end of the line\n",
    "               ], \n",
    "    header = 0,\n",
    "    names = ['label', 'text'],\n",
    "    lineterminator = '\\n'\n",
    ")\n",
    "\n",
    "# convert label from [1, 2] to [0, 1]\n",
    "trainDF['label'] = trainDF['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mILVIHomLtCf"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:24.213192Z",
     "start_time": "2019-06-17T01:39:24.209202Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "G9_8RbOeLtCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   9999 non-null   int64 \n",
      " 1   text    9999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>an absolute masterpiece: I am quite sure any o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  The best soundtrack ever to anything.: I'm rea...\n",
       "1      1  Amazing!: This soundtrack is my favorite music...\n",
       "2      1  Excellent Soundtrack: I truly like this soundt...\n",
       "3      1  Remember, Pull Your Jaw Off The Floor After He...\n",
       "4      1  an absolute masterpiece: I am quite sure any o..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YmYgG2pLtCu"
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:40.103737Z",
     "start_time": "2019-06-17T01:39:40.100739Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "j5vErjWFLtCy"
   },
   "outputs": [],
   "source": [
    "## ANSWER\n",
    "## split the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainDF['text'], trainDF['label'], test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nUp6oDOLtC1"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKd9yTnyLtC2"
   },
   "source": [
    "### Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:32.674674Z",
     "start_time": "2019-06-17T01:40:31.098889Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DU2RqqDjLtC3"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJs6al0ILtC5"
   },
   "source": [
    "### TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:36.088730Z",
     "start_time": "2019-06-17T01:40:34.519925Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "myjfdfP_LtC6",
    "outputId": "8bc4d529-1f66-4836-acd1-07633c29fd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 1 s\n",
      "Wall time: 995 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:57.505221Z",
     "start_time": "2019-06-17T01:40:49.387393Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-h16dUaVLtC_",
    "outputId": "3be1f8f5-670e-4249-8522-50509c8898a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 5.2 s\n",
      "Wall time: 5.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:41:10.209071Z",
     "start_time": "2019-06-17T01:40:59.211484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Y7rmIt49LtDC",
    "outputId": "8f600e7c-b4df-4d89-bfb9-96c8436aa5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3),\n",
      "                token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 7.5 s\n",
      "Wall time: 7.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         token_pattern = r'\\w{1,}',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Pck1cuvLtDH"
   },
   "source": [
    "### Text / NLP based features\n",
    "\n",
    "Create some other features.\n",
    "\n",
    "Char_Count = Number of Characters in Text\n",
    "\n",
    "Word Count = Number of Words in Text\n",
    "\n",
    "Word Density = Average Number of Char in Words\n",
    "\n",
    "Punctuation Count = Number of Punctuation in Text\n",
    "\n",
    "Title Word Count = Number of Words in Title\n",
    "\n",
    "Uppercase Word Count = Number of Upperwords in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:43:52.813132Z",
     "start_time": "2019-06-17T01:43:52.806150Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4jebGm1gLtDH",
    "outputId": "124d1ac4-9b64-414d-e973-9ded78662e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 940 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ANSWER\n",
    "\n",
    "trainDF['char_count'] = len(trainDF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(''.join(p for p in x if p in string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([w for w in x.split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['uupercase_word_count'] = trainDF['text'].apply(lambda x: len ([w for w in x.split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:44:03.442730Z",
     "start_time": "2019-06-17T01:44:02.298791Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Z-l2iZcLLtDO"
   },
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-9d0G59LtDR"
   },
   "source": [
    "Part of Speech in **SpaCy**\n",
    "\n",
    "    POS   DESCRIPTION               EXAMPLES\n",
    "    ----- ------------------------- ---------------------------------------------\n",
    "    ADJ   adjective                 big, old, green, incomprehensible, first\n",
    "    ADP   adposition                in, to, during\n",
    "    ADV   adverb                    very, tomorrow, down, where, there\n",
    "    AUX   auxiliary                 is, has (done), will (do), should (do)\n",
    "    CONJ  conjunction               and, or, but\n",
    "    CCONJ coordinating conjunction  and, or, but\n",
    "    DET   determiner                a, an, the\n",
    "    INTJ  interjection              psst, ouch, bravo, hello\n",
    "    NOUN  noun                      girl, cat, tree, air, beauty\n",
    "    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n",
    "    PART  particle                  's, not,\n",
    "    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n",
    "    PROPN proper noun               Mary, John, London, NATO, HBO\n",
    "    PUNCT punctuation               ., (, ), ?\n",
    "    SCONJ subordinating conjunction if, while, that\n",
    "    SYM   symbol                    $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù\n",
    "    VERB  verb                      run, runs, running, eat, ate, eating\n",
    "    X     other                     sfpksdpsxmsa\n",
    "    SPACE space\n",
    "    \n",
    "Find out number of Adjective, Adverb, Noun, Numeric, Pronoun, Proposition, Verb.\n",
    "\n",
    "    Hint:\n",
    "    1. Convert text to spacy document\n",
    "    2. Use pos_\n",
    "    3. Use Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:50:15.900377Z",
     "start_time": "2019-06-17T01:50:15.889406Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NcxmvIOGLtDS"
   },
   "outputs": [],
   "source": [
    "# Initialise some columns for feature's counts\n",
    "trainDF['adj_count'] = 0\n",
    "trainDF['adv_count'] = 0\n",
    "trainDF['noun_count'] = 0\n",
    "trainDF['num_count'] = 0\n",
    "trainDF['pron_count'] = 0\n",
    "trainDF['propn_count'] = 0\n",
    "trainDF['verb_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:52:39.611809Z",
     "start_time": "2019-06-17T01:52:39.608818Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1KfFtu1HcPwA"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "# for each text\n",
    "for i in range(trainDF.shape[0]):\n",
    "    # convert into a spaCy document\n",
    "    doc = nlp(trainDF.iloc[i]['text'])\n",
    "    # initialise feature counters\n",
    "    c = Counter([t.pos_ for t in doc])\n",
    "\n",
    "    trainDF.at[i, 'adj_count'] = c['ADJ']\n",
    "    trainDF.at[i, 'adv_count'] = c['ADV']\n",
    "    trainDF.at[i, 'noun_count'] = c['NOUN']\n",
    "    trainDF.at[i, 'num_count'] = c['NUM']\n",
    "    trainDF.at[i, 'pron_count'] = c['PRON']\n",
    "    trainDF.at[i, 'propn_count'] = c['PROPN']\n",
    "    trainDF.at[i, 'verb_count'] = c['VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:59:42.424828Z",
     "start_time": "2019-06-17T01:59:42.390920Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DW1_LKP2LtDX",
    "outputId": "7a5eb5fd-cae1-4e76-f95f-e0fe9f1780d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uupercase_word_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6582</th>\n",
       "      <td>9999</td>\n",
       "      <td>154</td>\n",
       "      <td>64.509677</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>9999</td>\n",
       "      <td>57</td>\n",
       "      <td>172.396552</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>9999</td>\n",
       "      <td>142</td>\n",
       "      <td>69.923077</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>9999</td>\n",
       "      <td>62</td>\n",
       "      <td>158.714286</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>9999</td>\n",
       "      <td>145</td>\n",
       "      <td>68.486301</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_count  word_count  word_density  punctuation_count  \\\n",
       "6582        9999         154     64.509677                 33   \n",
       "1590        9999          57    172.396552                 11   \n",
       "4265        9999         142     69.923077                 16   \n",
       "1712        9999          62    158.714286                 14   \n",
       "6689        9999         145     68.486301                 15   \n",
       "\n",
       "      title_word_count  uupercase_word_count  adj_count  adv_count  \\\n",
       "6582                25                    18         13          8   \n",
       "1590                 6                     3          2          1   \n",
       "4265                 9                     3         12          4   \n",
       "1712                 3                     1          4          2   \n",
       "6689                11                     9         14         11   \n",
       "\n",
       "      noun_count  num_count  pron_count  propn_count  verb_count  \n",
       "6582          28          2          22           10          22  \n",
       "1590          11          2           9            2           8  \n",
       "4265          34          2          15            6          20  \n",
       "1712          16          2           5            0          11  \n",
       "6689          25          2          26            1          19  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\n",
    "    'char_count', 'word_count', 'word_density',\n",
    "    'punctuation_count', 'title_word_count',\n",
    "    'uupercase_word_count', 'adj_count',\n",
    "    'adv_count', 'noun_count', 'num_count',\n",
    "    'pron_count', 'propn_count', 'verb_count']\n",
    "\n",
    "trainDF[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQCAUFWYLtDb"
   },
   "source": [
    "### Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:27:09.442903Z",
     "start_time": "2019-06-17T02:24:45.531924Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wg2mAlkRLtDb",
    "outputId": "323bfce4-9263-403a-9214-e9e206d5755f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 55 s\n",
      "Wall time: 55.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train a LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:28:11.804475Z",
     "start_time": "2019-06-17T02:28:10.978502Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_8dIDyHjLtDf",
    "outputId": "ea0614e2-66f1-4ddd-bff0-142cfd4fd78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Top Words\n",
      "----- --------------------------------------------------------------------------------\n",
      "    0 philadelphia condescending spoken tene schure bela serum fundamentalist lowest wagner\n",
      "    1 rock music vocals mad jimmy hostel cardboard eric acceptable acts\n",
      "    2 rip remembered heaven surgery julie patti phil emergency phrasing walker\n",
      "    3 kindle christmas lots season dialogue present show marquez kill diane\n",
      "    4 the of book and a is in to read this\n",
      "    5 laughs region et 19 gifts l tons les gems il\n",
      "    6 the i it and a to this is of for\n",
      "    7 reduce exuviance acne buffalo theology gerald pal undone rapidly fatal\n",
      "    8 anderson emma birds steer knot shades stalingrad recognition borred 7i\n",
      "    9 effects special house local states g4 hill haunting haunted jones\n",
      "   10 hollywood restaurant rome dinner definitly failing estate mild celiac precious\n",
      "   11 throughout cave cute romantic bear clan force hooked van tree\n",
      "   12 ear fit print large size printer jawbone quality fits noise\n",
      "   13 hp images costs taught tango hardware authentic fooled sounding highest\n",
      "   14 cap fairy trade gillian amy incomplete circuit gary witch custom\n",
      "   15 ballet scooter dali yea oriented drip frm controller questo garp\n",
      "   16 music la de cd u eat per y nowhere henry\n",
      "   17 nearly moment higgins legs con richard discover flaws kate gain\n",
      "   18 451 bradbury fahrenheit future ray montag guy television peace controls\n",
      "   19 tracks max album tape provide info scanner practice theory asleep\n"
     ]
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtfnK1jeLtDl"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:12.273365Z",
     "start_time": "2019-06-17T02:34:12.263393Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uwVaWSyTLtDm"
   },
   "outputs": [],
   "source": [
    "## helper function\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    return accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:14.900001Z",
     "start_time": "2019-06-17T02:34:14.894016Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "f_onpqUkLtDo"
   },
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXwLriDpLtDq"
   },
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:34.147043Z",
     "start_time": "2019-06-17T02:34:34.123096Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZcU6IKyNLtDs",
    "outputId": "d2defcfb-2046-47e1-b3b6-08d6edca94f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors    : 0.8540\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print('NB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:36.399812Z",
     "start_time": "2019-06-17T02:34:36.381861Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zqEG_ByTLtDv",
    "outputId": "0ba74f49-a71c-4d59-f57b-f39f546241ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF : 0.8600\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:39.076000Z",
     "start_time": "2019-06-17T02:34:39.059047Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uKEEPNi8LtDy",
    "outputId": "c00cf532-c914-4670-cde5-69bf54bf7201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors   : 0.8400\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 7.08 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:42.057019Z",
     "start_time": "2019-06-17T02:34:42.009151Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "M9aIibkBLtD0",
    "outputId": "1f52c1f4-150d-4195-f927-cb66ec41baba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors: 0.8180\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 28.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:46.265712Z",
     "start_time": "2019-06-17T02:34:46.258734Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kkrodUzCLtD3"
   },
   "outputs": [],
   "source": [
    "results.loc['Na√Øve Bayes'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2oNfajULtD4"
   },
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:50.841687Z",
     "start_time": "2019-06-17T02:34:48.637032Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OFBhhPZ6LtD4",
    "outputId": "cb5a2b19-dfc1-4b11-e698-0488ba4eae53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors    : 0.8520\n",
      "\n",
      "CPU times: total: 5.66 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n",
    "print('LR, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.310433Z",
     "start_time": "2019-06-17T02:34:51.214690Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "C89hRhDiLtD6",
    "outputId": "023669b1-788f-4a1b-d282-99464e26312c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF : 0.8730\n",
      "\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 63.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.749258Z",
     "start_time": "2019-06-17T02:34:51.683435Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MvhV1jC6LtD9",
    "outputId": "9bef9b1d-2dc3-4300-c4ff-831957aadee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors   : 0.8360\n",
      "\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 47.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:52.635899Z",
     "start_time": "2019-06-17T02:34:52.175122Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XPjIxmtKLtEA",
    "outputId": "58f4b0e9-e786-45a1-830e-5945129792d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors: 0.8485\n",
      "\n",
      "CPU times: total: 1.06 s\n",
      "Wall time: 343 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:53.029844Z",
     "start_time": "2019-06-17T02:34:53.018872Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZFK_LWTcLtED"
   },
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1wYto68LtEE"
   },
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.237613Z",
     "start_time": "2019-06-17T02:34:53.406835Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yYGz8he5LtEE",
    "outputId": "546b1ea8-27c9-45bb-fd91-da08244d6796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors    : 0.8345\n",
      "\n",
      "CPU times: total: 719 ms\n",
      "Wall time: 404 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.743263Z",
     "start_time": "2019-06-17T02:34:54.606629Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wMt26K0cLtEG",
    "outputId": "406cf62d-c09d-4f9b-c298-eabce0b69238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF : 0.8610\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:55.220003Z",
     "start_time": "2019-06-17T02:34:55.119256Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eFt6Y1VvLtEI",
    "outputId": "628299db-8bca-4698-c64e-5b291bd248e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors   : 0.8210\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 32.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.139528Z",
     "start_time": "2019-06-17T02:34:55.585010Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iqhsS579LtEL",
    "outputId": "7cc3d723-9858-43a8-dcdf-37fcbef9456e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors: 0.8570\n",
      "\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 373 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.501558Z",
     "start_time": "2019-06-17T02:34:56.492592Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "go0bcKeILtEN"
   },
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLGxWK0yLtEO"
   },
   "source": [
    "### Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:20.807157Z",
     "start_time": "2019-06-17T02:34:56.823697Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HR8aOytWLtEO",
    "outputId": "961d5bca-c1fe-46be-ba8c-2f6325d85901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors    : 0.8245\n",
      "\n",
      "CPU times: total: 13.2 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n",
    "print('RF, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:30.127233Z",
     "start_time": "2019-06-17T02:35:21.198110Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zXcTCTEGLtET",
    "outputId": "e4336045-c508-4372-e102-35c559f9a195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF : 0.8300\n",
      "\n",
      "CPU times: total: 6.08 s\n",
      "Wall time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:40.162390Z",
     "start_time": "2019-06-17T02:35:30.607944Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EnvT8qvSLtEW",
    "outputId": "0c3a8fa0-7ee7-40ad-dc0e-85a8c3995733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors   : 0.7830\n",
      "\n",
      "CPU times: total: 5.62 s\n",
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:07.640904Z",
     "start_time": "2019-06-17T02:35:40.542371Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8jt-dTVELtEX",
    "outputId": "dc1113c9-b3f2-4d2e-f4d7-a5bcc98a020c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, CharLevel Vectors: 0.7865\n",
      "\n",
      "CPU times: total: 22.9 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:08.104108Z",
     "start_time": "2019-06-17T02:36:08.097127Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fVKeCH_VLtEZ"
   },
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyVz4Q6ILtEa"
   },
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:37.197296Z",
     "start_time": "2019-06-17T02:36:08.451184Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8wGvHTg-LtEb",
    "outputId": "dcc31f90-b0f8-4f5d-c835-25be80638e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, Count Vectors    : 0.7990\n",
      "\n",
      "CPU times: total: 27.8 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n",
    "print('GB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:52.852454Z",
     "start_time": "2019-06-17T02:36:37.714920Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HJNwQf57LtEd",
    "outputId": "686abbc5-8745-4dca-c2e2-b665b9d19901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, WordLevel TF-IDF : 0.7955\n",
      "\n",
      "CPU times: total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:37:02.608379Z",
     "start_time": "2019-06-17T02:36:53.252355Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iyPqLMgkLtEe",
    "outputId": "b7c87e04-f724-4ccf-e02e-28beb7e40654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, N-Gram Vectors   : 0.7345\n",
      "\n",
      "CPU times: total: 6.08 s\n",
      "Wall time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.314194Z",
     "start_time": "2019-06-17T02:37:03.039224Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1KYdyatTLtEg",
    "outputId": "aa4f303e-76ff-47e6-a04f-7e311e1848ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, CharLevel Vectors: 0.8020\n",
      "\n",
      "CPU times: total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.683222Z",
     "start_time": "2019-06-17T02:39:14.675213Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AC0hWO59LtEj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:15.024279Z",
     "start_time": "2019-06-17T02:39:15.010319Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "b9fY4J7XLtEk",
    "outputId": "ea4a8ddf-5b37-4cb9-da79-1920e1967b03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Vectors</th>\n",
       "      <th>WordLevel TF-IDF</th>\n",
       "      <th>N-Gram Vectors</th>\n",
       "      <th>CharLevel Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Na√Øve Bayes</th>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.8180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.8485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>0.8210</td>\n",
       "      <td>0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.7830</td>\n",
       "      <td>0.7865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.7955</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>0.8020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n",
       "Na√Øve Bayes                    0.8540            0.8600          0.8400   \n",
       "Logistic Regression            0.8520            0.8730          0.8360   \n",
       "Support Vector Machine         0.8345            0.8610          0.8210   \n",
       "Random Forest                  0.8245            0.8300          0.7830   \n",
       "Gradient Boosting              0.7990            0.7955          0.7345   \n",
       "\n",
       "                        CharLevel Vectors  \n",
       "Na√Øve Bayes                        0.8180  \n",
       "Logistic Regression                0.8485  \n",
       "Support Vector Machine             0.8570  \n",
       "Random Forest                      0.7865  \n",
       "Gradient Boosting                  0.8020  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > ¬© 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Pck1cuvLtDH",
    "mQCAUFWYLtDb",
    "OXwLriDpLtDq",
    "-2oNfajULtD4",
    "q1wYto68LtEE",
    "gLGxWK0yLtEO"
   ],
   "name": "IOD_Lab-9_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
